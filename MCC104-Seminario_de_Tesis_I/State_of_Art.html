<meta http-equiv="Content-Type" content="text/html; charset=utf-8"><link type="text/css" rel="stylesheet" href="resources/sheet.css" >
<style type="text/css">.ritz .waffle a { color: inherit; }.ritz .waffle .s21{border-right:1px SOLID #000000;background-color:#ffff00;text-align:right;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s0{border-bottom:1px SOLID #000000;background-color:#ffffff;}.ritz .waffle .s14{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s1{border-right:1px SOLID #000000;background-color:#ffffff;}.ritz .waffle .s16{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;font-weight:bold;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s3{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s5{border-right:1px SOLID #000000;background-color:#ffffff;text-align:right;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s12{background-color:#ffffff;text-align:left;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s19{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s18{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;font-weight:bold;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s10{background-color:#ffffff;text-align:center;font-weight:bold;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s6{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;color:#ce181e;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s15{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;font-weight:bold;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;color:#1155cc;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s22{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;font-weight:bold;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s23{background-color:#ffffff;text-align:left;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s8{background-color:#ffffff;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s7{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;font-weight:bold;color:#ce181e;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s9{border-bottom:1px SOLID #000000;background-color:#ffffff;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s11{background-color:#ffffff;text-align:right;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s2{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;font-weight:bold;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s4{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ffffff;text-align:center;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;color:#1155cc;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s13{border-bottom:1px SOLID #000000;background-color:#ffffff;text-align:left;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s20{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;font-weight:bold;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;color:#1155cc;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:normal;overflow:hidden;word-wrap:break-word;direction:ltr;padding:0px 3px 0px 3px;}.ritz .waffle .s17{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;background-color:#ea9999;text-align:center;color:#000000;font-family:'Arial';font-size:10pt;vertical-align:middle;white-space:nowrap;direction:ltr;padding:0px 3px 0px 3px;}</style><div class="ritz grid-container" dir="ltr"><table class="waffle" cellspacing="0" cellpadding="0"><thead><tr><th class="row-header freezebar-origin-ltr"></th><th id="254117349C0" style="width:33px" class="column-headers-background">A</th><th id="254117349C1" style="width:397px" class="column-headers-background">B</th><th id="254117349C2" style="width:124px" class="column-headers-background">C</th><th id="254117349C3" style="width:80px" class="column-headers-background">D</th><th id="254117349C4" style="width:80px" class="column-headers-background">E</th><th id="254117349C5" style="width:80px" class="column-headers-background">F</th><th id="254117349C6" style="width:80px" class="column-headers-background">G</th><th id="254117349C7" style="width:80px" class="column-headers-background">H</th><th id="254117349C8" style="width:80px" class="column-headers-background">I</th><th id="254117349C9" style="width:66px" class="column-headers-background">J</th><th id="254117349C10" style="width:71px" class="column-headers-background">K</th><th id="254117349C11" style="width:72px" class="column-headers-background">L</th><th id="254117349C12" style="width:64px" class="column-headers-background">M</th><th id="254117349C13" style="width:92px" class="column-headers-background">N</th><th id="254117349C14" style="width:88px" class="column-headers-background">O</th><th id="254117349C15" style="width:80px" class="column-headers-background">P</th><th id="254117349C16" style="width:93px" class="column-headers-background">Q</th><th id="254117349C17" style="width:72px" class="column-headers-background">R</th></tr></thead><tbody><tr style='height:16px;'><th id="254117349R0" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">1</div></th><td></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td></td></tr><tr style='height:16px;'><th id="254117349R1" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">2</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="16" rowspan="2">Object Detection (Bounding Box)</td><td></td></tr><tr style='height:16px;'><th id="254117349R2" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">3</div></th><td class="s1"></td><td></td></tr><tr style='height:16px;'><th id="254117349R3" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">4</div></th><td class="s1"></td><td class="s3" rowspan="2">Method</td><td class="s3" dir="ltr" rowspan="2">Nombre</td><td class="s3" rowspan="2">Año</td><td class="s3" dir="ltr" colspan="8">Backbone - Feature Extractor</td><td class="s3" dir="ltr" colspan="3">Region Proposals</td><td class="s3" colspan="2">Dataset</td><td></td></tr><tr style='height:16px;'><th id="254117349R4" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">5</div></th><td class="s1"></td><td class="s3">ResNet</td><td class="s3">VGGNet</td><td class="s3">Inception</td><td class="s3">GoogleNet</td><td class="s3">MobileNet</td><td class="s3">AlexNet</td><td class="s3" dir="ltr">ZFNet</td><td class="s3">DarkNet</td><td class="s3">RPN</td><td class="s3">FCN</td><td class="s4"><a target="_blank" href="https://arxiv.org/pdf/1612.03144.pdf">FPN</a></td><td class="s3">Pascal VOC</td><td class="s3">COCO</td><td></td></tr><tr style='height:16px;'><th id="254117349R5" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">6</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1311.2524.pdf">Rich feature hierarchies for accurate object detection and semantic segmentation; R-CNN</a></td><td class="s3" dir="ltr">R-CNN</td><td class="s3">2014</td><td class="s3"></td><td class="s3" dir="ltr">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R6" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">7</div></th><td class="s5">2</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1406.4729.pdf">SPPNet: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></td><td class="s3" dir="ltr">SPP-Net</td><td class="s3" dir="ltr">2014</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3" dir="ltr">X</td><td class="s3" dir="ltr">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3" dir="ltr">X</td><td class="s3" dir="ltr"></td><td></td></tr><tr style='height:16px;'><th id="254117349R7" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">8</div></th><td class="s5">3</td><td class="s4"><a target="_blank" href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN</a></td><td class="s3" dir="ltr">Fast R-CNN</td><td class="s3">2015</td><td class="s3"></td><td class="s3">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R8" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">9</div></th><td class="s5">4</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1506.02640v5.pdf">You Only Look Once (YOLO):<br>Unified, Real-Time Object Detection</a></td><td class="s3" dir="ltr">YOLO</td><td class="s3">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s6">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R9" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">10</div></th><td class="s5">5</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td><td class="s3" dir="ltr">Faster R-CNN</td><td class="s3">2016</td><td class="s6">X</td><td class="s3">X</td><td class="s6">X</td><td class="s3">X</td><td class="s3">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R10" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">11</div></th><td class="s5">6</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1512.02325.pdf">SSD: Single Shot MultiBox Detector</a></td><td class="s3" dir="ltr">SSD</td><td class="s3">2016</td><td class="s3">X</td><td class="s3">X</td><td class="s3">X</td><td class="s3">X</td><td class="s6">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R11" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">12</div></th><td class="s5">7</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1605.06409.pdf">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a></td><td class="s3" dir="ltr">R-FCN</td><td class="s3">2016</td><td class="s6">X</td><td class="s3">X</td><td class="s6">X</td><td class="s3">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R12" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">13</div></th><td class="s5">8</td><td class="s4" dir="ltr"><a target="_blank" href="https://pjreddie.com/media/files/papers/YOLO9000.pdf">YOLO9000: Better, Faster, Stronger</a></td><td class="s3" dir="ltr">YOLO9000</td><td class="s3">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s6">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R13" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">14</div></th><td class="s5">9</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1612.03144.pdf">Feature Pyramid Networks for Object Detection</a></td><td class="s3" dir="ltr">FPN</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s6"></td><td class="s3"></td><td class="s3"></td><td class="s3" dir="ltr">X</td><td class="s3"></td><td class="s3"></td><td></td></tr><tr style='height:16px;'><th id="254117349R14" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">15</div></th><td class="s5">10</td><td class="s4"><a target="_blank" href="https://arxiv.org/pdf/1708.02002.pdf">Retina-Net: Focal Loss for Dense Object Detection</a></td><td class="s3" dir="ltr">Retina-Net</td><td class="s3">2017</td><td class="s7">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R15" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">16</div></th><td class="s5">11</td><td class="s4" dir="ltr"><a target="_blank" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLOv3: An Incremental Improvement</a></td><td class="s3" dir="ltr">YOLOv3</td><td class="s3">2018</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s6">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R16" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">17</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td class="s3" dir="ltr">YOLOv4</td><td class="s3" dir="ltr">2020</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3">X</td><td class="s3">X</td><td></td></tr><tr style='height:16px;'><th id="254117349R17" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">18</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R18" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">19</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R19" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">20</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R20" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">21</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R21" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">22</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R22" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">23</div></th><td></td><td class="s9"></td><td class="s9"></td><td class="s9"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R23" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">24</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="3" rowspan="2">Convolutional Neural Network Models</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R24" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">25</div></th><td class="s1"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R25" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">26</div></th><td class="s1"></td><td class="s3" rowspan="2">Paper</td><td class="s3" rowspan="2">Nombre</td><td class="s3" rowspan="2">Año</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R26" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">27</div></th><td class="s1"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R27" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">28</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf">Handwritten Digit Recognition with a Back-Propagation Network</a></td><td class="s3" dir="ltr">LeNet</td><td class="s3" dir="ltr">1989</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R28" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">29</div></th><td class="s5">2</td><td class="s4" dir="ltr"><a target="_blank" href="http://yann.lecun.com/exdb/publis/pdf/lecun-95b.pdf">Comparison of Learning Algorithms for Handwritten Digit Recognition</a></td><td class="s3" dir="ltr">LeNet-5</td><td class="s3" dir="ltr">1995</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R29" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">30</div></th><td class="s5">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></td><td class="s3" dir="ltr">AlexNet</td><td class="s3" dir="ltr">2012</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R30" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">31</div></th><td class="s5">4</td><td class="s4" dir="ltr"><a target="_blank" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">Visualizing and Understanding Convolutional Networks</a></td><td class="s3" dir="ltr">ZF-Net</td><td class="s3" dir="ltr">2013</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R31" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">32</div></th><td class="s5">5</td><td class="s4" dir="ltr"><a target="_blank" href="https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf">Caffe: Convolutional Architecture for Fast Feature Embedding</a></td><td class="s3" dir="ltr">CaffeNet</td><td class="s3" dir="ltr">2014</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R32" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">33</div></th><td class="s5">6</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1409.4842.pdf">Going deeper with convolutions</a></td><td class="s3" dir="ltr">GoogleNet (InceptionV1)</td><td class="s3">2014</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R33" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">34</div></th><td class="s5">7</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td><td class="s3">VGG-Net</td><td class="s3">2014</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R34" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">35</div></th><td class="s5">8</td><td class="s4" dir="ltr"><a target="_blank" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a></td><td class="s3" dir="ltr">ResNet</td><td class="s3" dir="ltr">2015</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R35" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">36</div></th><td class="s5">9</td><td class="s4" dir="ltr"><a target="_blank" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf">Rethinking the Inception Architecture for Computer Vision</a></td><td class="s3" dir="ltr">InceptionV3</td><td class="s3" dir="ltr">2015</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R36" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">37</div></th><td class="s5">10</td><td class="s4" dir="ltr"><a target="_blank" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">Siamese Neural Networks for One-shot Image Recognition</a></td><td class="s3" dir="ltr">SiameseNet</td><td class="s3" dir="ltr">2015</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R37" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">38</div></th><td class="s5">11</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep Residual Networks</a></td><td class="s3" dir="ltr">ResNet-v2</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R38" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">39</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1611.05431.pdf">Aggregated Residual Transformations for Deep Neural Networks</a></td><td class="s3" dir="ltr">ResNeXt</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R39" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">40</div></th><td class="s5">13</td><td class="s4" dir="ltr"><a target="_blank" href="https://pjreddie.com/media/files/papers/xnor.pdf">XNOR-Net: ImageNet Classification Using Binary<br>Convolutional Neural Networks</a></td><td class="s3" dir="ltr">XOR-Net</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R40" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">41</div></th><td class="s5">14</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1610.02357.pdf">Xception: Deep Learning with Depthwise Separable Convolutions</a></td><td class="s3" dir="ltr">Xception</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R41" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">42</div></th><td class="s5">15</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1602.07261.pdf">Inception-ResNet and the Impact of Residual Connections on Learning</a></td><td class="s3" dir="ltr">InceptionV4</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R42" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">43</div></th><td class="s5">16</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1709.01507.pdf">Squeeze-and-Excitation Networks</a></td><td class="s3" dir="ltr">SE-Net</td><td class="s3" dir="ltr">2017</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R43" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">44</div></th><td class="s5">17</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1704.04861.pdf">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td><td class="s3" dir="ltr">MobileNet</td><td class="s3" dir="ltr">2017</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R44" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">45</div></th><td class="s5">18</td><td class="s4" dir="ltr"><a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">Densely Connected Convolutional Networks</a></td><td class="s3" dir="ltr">Dense-Net</td><td class="s3" dir="ltr">2017</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R45" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">46</div></th><td class="s5">19</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1801.04381.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td><td class="s3" dir="ltr">MobileNet v2</td><td class="s3" dir="ltr">2018</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R46" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">47</div></th><td class="s5">20</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1703.09844.pdf">Multi-Scale Dense Networks for Resource Efficient Image Classification</a></td><td class="s3" dir="ltr">MSD-Net</td><td class="s3" dir="ltr">2018</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R47" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">48</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R48" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">49</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R49" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">50</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R50" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">51</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R51" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">52</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R52" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">53</div></th><td></td><td class="s9" dir="ltr"></td><td class="s9" dir="ltr"></td><td class="s9"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R53" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">54</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="3" rowspan="2">Datasets</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R54" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">55</div></th><td class="s1"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R55" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">56</div></th><td class="s1"></td><td class="s3" dir="ltr" rowspan="2">Paper</td><td class="s3" dir="ltr" rowspan="2">Nombre</td><td class="s3" rowspan="2">Año</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R56" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">57</div></th><td class="s1"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R57" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">58</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="http://vision.stanford.edu/pdf/ImageNet_CVPR2009.pdf">ImageNet: A Large-Scale Hierarchical Image Database</a></td><td class="s3" dir="ltr">ImageNet</td><td class="s3" dir="ltr">2009</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R58" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">59</div></th><td class="s5" dir="ltr">2</td><td class="s4" dir="ltr"><a target="_blank" href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf">The Pascal Visual Object Classes (VOC) Challenge</a></td><td class="s3" dir="ltr">Pascal-VOC</td><td class="s3" dir="ltr">2010</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R59" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">60</div></th><td class="s5" dir="ltr">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://www.cc.gatech.edu/~hays/papers/sun.pdf">SUN Database: Large-scale Scene Recognition from Abbey to Zoo</a></td><td class="s3" dir="ltr">SUN</td><td class="s3" dir="ltr">2010</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R60" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">61</div></th><td class="s5" dir="ltr">4</td><td class="s4" dir="ltr"><a target="_blank" href="http://cs.brown.edu/people/gmpatter/pub_papers/SUN_Attribute_Database_CVPR2012.pdf">SUN Attribute Database:<br>Discovering, Annotating, and Recognizing Scene Attributes</a></td><td class="s3" dir="ltr">SUN-Attr</td><td class="s3" dir="ltr">2012</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R61" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">62</div></th><td class="s5" dir="ltr">5</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1405.0312.pdf">Microsoft COCO: Common Objects in Context</a></td><td class="s3" dir="ltr">COCO</td><td class="s3" dir="ltr">2014</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R62" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">63</div></th><td class="s5" dir="ltr">6</td><td class="s4" dir="ltr"><a target="_blank" href="https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf">The Role of Context for Object Detection and Semantic Segmentation in the Wild</a></td><td class="s3" dir="ltr">PASCAL Context</td><td class="s3" dir="ltr">2014</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R63" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">64</div></th><td class="s5" dir="ltr">7</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1604.01685.pdf">The Cityscapes Dataset for Semantic Urban Scene Understanding</a></td><td class="s3" dir="ltr">CitySpaces</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R64" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">65</div></th><td class="s5" dir="ltr">8</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1610.02055.pdf">Places: An Image Database for Deep Scene Understanding</a></td><td class="s3" dir="ltr">Places205</td><td class="s3" dir="ltr">2016</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R65" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">66</div></th><td class="s5" dir="ltr">9</td><td class="s4" dir="ltr"><a target="_blank" href="http://places2.csail.mit.edu/PAMI_places.pdf">Places2: A 10 million Image Database for Scene Recognition</a></td><td class="s3" dir="ltr">Places365</td><td class="s3" dir="ltr">2017</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R66" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">67</div></th><td class="s5" dir="ltr">10</td><td class="s4" dir="ltr"><a target="_blank" href="http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf">ADE20k: Scene Parsing through ADE20K Dataset</a></td><td class="s3" dir="ltr">ADE20K</td><td class="s3" dir="ltr">2017</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R67" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">68</div></th><td class="s5" dir="ltr">11</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1811.00982.pdf">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</a></td><td class="s3" dir="ltr">OpenImages</td><td class="s3" dir="ltr">2018</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R68" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">69</div></th><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R69" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">70</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R70" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">71</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td></td><td></td><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R71" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">72</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td></td><td></td><td></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R72" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">73</div></th><td></td><td class="s8" dir="ltr"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R73" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">74</div></th><td></td><td class="s9" dir="ltr"></td><td class="s9" dir="ltr"></td><td class="s9"></td><td class="s9"></td><td class="s9"></td><td class="s9"></td><td class="s9"></td><td class="s9"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td></tr><tr style='height:16px;'><th id="254117349R74" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">75</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="8" rowspan="2">Instance and Semantic Segmentation (Pixel Mask)</td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td></tr><tr style='height:16px;'><th id="254117349R75" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">76</div></th><td class="s1"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td><td class="s10" dir="ltr"></td></tr><tr style='height:16px;'><th id="254117349R76" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">77</div></th><td class="s1"></td><td class="s3" rowspan="2">Method</td><td class="s3" dir="ltr" rowspan="2">Nombre</td><td class="s3" rowspan="2">Año</td><td class="s3" colspan="5">Dataset</td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R77" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">78</div></th><td class="s1"></td><td class="s3">Pascal VOC</td><td class="s3">COCO</td><td class="s3" dir="ltr">CitySpace</td><td class="s3" dir="ltr">NYU Depth</td><td class="s3" dir="ltr">PASCAL Context</td><td class="s8"></td><td class="s8" dir="ltr"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R78" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">79</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="https://people.csail.mit.edu/dsontag/papers/SilSonFer_ECCV14.pdf">Instance Segmentation of Indoor Scenes using a Coverage Loss</a></td><td class="s3"></td><td class="s3" dir="ltr">2014</td><td class="s3">X</td><td class="s3">X</td><td class="s3"></td><td class="s3" dir="ltr">X</td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R79" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">80</div></th><td class="s5">2</td><td class="s4" dir="ltr"><a target="_blank" href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">Fully Convolutional Networks for Semantic Segmentation</a></td><td class="s3" dir="ltr">FCN</td><td class="s3" dir="ltr">2014</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R80" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">81</div></th><td class="s11">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1407.1808.pdf">Simultaneous Detection and Segmentation</a></td><td class="s3" dir="ltr">SDS</td><td class="s3" dir="ltr">2014</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R81" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">82</div></th><td class="s5">4</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1506.04579.pdf">ParseNet: Looking Wider to See Better</a></td><td class="s3" dir="ltr">ParseNet</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R82" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">83</div></th><td class="s5">5</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1505.04366.pdf">Learning Deconvolution Network for Semantic Segmentation</a></td><td class="s3" dir="ltr">DeconvNet</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R83" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">84</div></th><td class="s5">6</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1505.04597.pdf">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></td><td class="s3" dir="ltr">U-Net</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R84" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">85</div></th><td class="s5">7</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1506.06204.pdf">Learning to Segment Object Candidates</a></td><td class="s3" dir="ltr">DeepMask</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R85" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">86</div></th><td class="s5">8</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1411.5752.pdf">Hypercolumns for Object Segmentation and Fine-grained Localization</a></td><td class="s3" dir="ltr">Hypercolumn</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R86" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">87</div></th><td class="s5">9</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1511.00561.pdf">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></td><td class="s3" dir="ltr">SegNet</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R87" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">88</div></th><td class="s5">10</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1512.04412.pdf">Instance-aware Semantic Segmentation via Multi-task Network Cascades</a></td><td class="s3" dir="ltr">MNC</td><td class="s3" dir="ltr">2015</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R88" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">89</div></th><td class="s5">11</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1511.04517.pdf">Reversible Recursive Instance-level Object Segmentation</a></td><td class="s3" dir="ltr">R2-IOS</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R89" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">90</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1511.08250.pdf">Recurrent Instance Segmentation</a></td><td class="s3" dir="ltr">RIS</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R90" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">91</div></th><td class="s5">13</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1511.07122.pdf">Multi-Scale Context Aggregation by Dilated Convolutions</a></td><td class="s3" dir="ltr">DilatedNet</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R91" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">92</div></th><td class="s5">14</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1509.02636.pdf">Proposal-free Network for Instance-level Object Segmentation</a></td><td class="s3" dir="ltr">PFN</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R92" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">93</div></th><td class="s5">15</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1612.01105.pdf">Pyramid Scene Parsing Network</a></td><td class="s3" dir="ltr">PSPNet</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R93" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">94</div></th><td class="s5">16</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1603.08695.pdf">Learning to Refine Object Segments</a></td><td class="s3" dir="ltr">SharpMask</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R94" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">95</div></th><td class="s5">17</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1603.08678.pdf">Instance-sensitive Fully Convolutional Networks</a></td><td class="s3" dir="ltr">InstanceFCN</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R95" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">96</div></th><td class="s5">18</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1604.02135.pdf">A MultiPath Network for Object Detection</a></td><td class="s3" dir="ltr">MPN</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R96" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">97</div></th><td class="s5">19</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1611.06612.pdf">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</a></td><td class="s3" dir="ltr">RefineNet</td><td class="s3" dir="ltr">2016</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R97" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">98</div></th><td class="s5">20</td><td class="s4" dir="ltr"><a target="_blank" href="https://papers.nips.cc/paper/6636-maskrnn-instance-level-video-object-segmentation.pdf">MaskRNN: Instance Level Video Object Segmentation</a></td><td class="s3" dir="ltr">MaskRNN</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R98" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">99</div></th><td class="s5">21</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1705.09914.pdf">Dilated residual networks</a></td><td class="s3" dir="ltr">DRN</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R99" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">100</div></th><td class="s5">22</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1611.07709.pdf">Fully Convolutional Instance-aware Semantic Segmentation</a></td><td class="s3" dir="ltr">FCIS</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R100" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">101</div></th><td class="s5">23</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1606.00915.pdf">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></td><td class="s3" dir="ltr">DeepLab</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R101" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">102</div></th><td class="s5">24</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1706.05587.pdf">Rethinking Atrous Convolution for Semantic Image Segmentation</a></td><td class="s3" dir="ltr">DeepLabv3</td><td class="s3" dir="ltr">2017</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R102" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">103</div></th><td class="s5">25</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a></td><td class="s3">Mask R-CNN</td><td class="s3">2018</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R103" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">104</div></th><td class="s5">26</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1803.08904.pdf">Context Encoding for Semantic Segmentation</a></td><td class="s3" dir="ltr">EncNet</td><td class="s3" dir="ltr">2018</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R104" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">105</div></th><td class="s5">27</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1803.01534.pdf">Path Aggregation Network for Instance Segmentation</a></td><td class="s3" dir="ltr">PANet</td><td class="s3" dir="ltr">2018</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R105" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">106</div></th><td class="s5">28</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1903.00241.pdf">Mask Scoring R-CNN</a></td><td class="s3" dir="ltr">Ms-RCNN</td><td class="s3" dir="ltr">2019</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R106" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">107</div></th><td class="s5">29</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1901.03784.pdf">UPSNet: A Unified Panoptic Segmentation Network</a></td><td class="s3" dir="ltr">UPSNet</td><td class="s3" dir="ltr">2019</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td class="s8"></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R107" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">108</div></th><td class="s5">30</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1904.02689.pdf">YOLACT: Real-time Instance Segmentation</a></td><td class="s3" dir="ltr">YOLACT</td><td class="s3" dir="ltr">2019</td><td class="s3" dir="ltr">X</td><td class="s3" dir="ltr">X</td><td class="s3"></td><td class="s3"></td><td class="s3"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R108" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">109</div></th><td class="s12" dir="ltr"> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R109" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">110</div></th><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R110" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">111</div></th><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R111" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">112</div></th><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R112" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">113</div></th><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R113" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">114</div></th><td></td><td class="s0"></td><td class="s13" dir="ltr"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td></tr><tr style='height:16px;'><th id="254117349R114" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">115</div></th><td class="s1"></td><td class="s2" colspan="17" rowspan="2">Urban Perception</td></tr><tr style='height:16px;'><th id="254117349R115" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">116</div></th><td class="s1"></td></tr><tr style='height:16px;'><th id="254117349R116" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">117</div></th><td class="s1"></td><td class="s14" dir="ltr" rowspan="2">Paper</td><td class="s14" rowspan="2">Año</td><td class="s14" colspan="6" rowspan="2">Feature Extractor Technique</td><td class="s14" colspan="2" rowspan="2">Ranking Classifier</td><td class="s14" rowspan="2">Predictor</td><td class="s14" rowspan="2">Acc (%)</td><td class="s14" rowspan="2">Dataset</td><td class="s14" colspan="4" rowspan="2">Contribution</td></tr><tr style='height:16px;'><th id="254117349R117" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">118</div></th><td class="s1"></td></tr><tr style='height:16px;'><th id="254117349R118" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">119</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="http://graphics.cs.cmu.edu/projects/whatMakesParis/paris_sigg_reduced.pdf">What makes paris look like paris?; Doersch</a></td><td class="s14" dir="ltr">2012</td><td class="s14" dir="ltr" colspan="6">SIFT, HOG to extract visual words, training with k-means</td><td class="s14" colspan="2"></td><td class="s14" dir="ltr">SVM</td><td class="s14"></td><td class="s3"></td><td class="s3" dir="ltr" colspan="4">Give an idea about how to correlate images of cities and similarities</td></tr><tr style='height:16px;'><th id="254117349R119" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">120</div></th><td class="s5">2</td><td class="s15" dir="ltr"><a target="_blank" href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0068400&amp;type=printable">The Collaborative Image of The City: Mapping the<br>Inequality of Urban Perception; Salesse</a></td><td class="s16">2013</td><td class="s17" colspan="6">Analyse and ponderate perception score from survey.</td><td class="s17" colspan="2">None</td><td class="s17">None</td><td class="s17">None</td><td class="s18">PlacePulse 1.0</td><td class="s19" colspan="4">Creates PlacePulse Dataset</td></tr><tr style='height:16px;'><th id="254117349R120" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">121</div></th><td class="s5">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://hcramer.files.wordpress.com/2014/02/cscw2014-querciaoharecramer.pdf">Aesthetic capital: what makes London look beautiful, quiet, and happy?; Quercia</a></td><td class="s14" dir="ltr">2014</td><td class="s14" dir="ltr" colspan="6">MPEG-7 Edge Histogram descriptor to extract visual words with K-means</td><td class="s14" colspan="2"></td><td class="s3" dir="ltr">Linear Regression</td><td class="s14"></td><td class="s3"></td><td class="s3" dir="ltr" colspan="4">Present points in images selected by people to classify into beauty, quiet and happy</td></tr><tr style='height:16px;'><th id="254117349R121" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">122</div></th><td class="s5">4</td><td class="s15" dir="ltr"><a target="_blank" href="http://vicenteordonez.com/urban/vicente_eccv14.pdf">Learning High-level Judgments of<br>Urban Perception; Ordoñez</a></td><td class="s16" dir="ltr">2014</td><td class="s17" colspan="6">DeCAF, Fisher Vector, GIST</td><td class="s17" colspan="2">SVM</td><td class="s17">SVM</td><td class="s17">69</td><td class="s18">PlacePulse 1.0</td><td class="s19" dir="ltr" colspan="4">Classification and regression of scores </td></tr><tr style='height:16px;'><th id="254117349R122" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">123</div></th><td class="s5">5</td><td class="s4" dir="ltr"><a target="_blank" href="http://vis.berkeley.edu/papers/cityforensics/paper.pdf">City Forensics: Using Visual Elements to Predict<br>Non-Visual City Attributes; Arietta</a></td><td class="s14" dir="ltr">2014</td><td class="s14" dir="ltr" colspan="6">CaffeNet, HOG+color to extract visual elements</td><td class="s14" dir="ltr" colspan="2">SVM</td><td class="s14" dir="ltr">SVR</td><td class="s14"></td><td class="s3" dir="ltr">Crime database, graffitis db, tree db</td><td class="s3" dir="ltr" colspan="4">framework to predict relationships between the visual appearance of a city and its non-visual attributes (e.g. crime statistics, housing prices, population density etc.). </td></tr><tr style='height:16px;'><th id="254117349R123" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">124</div></th><td class="s5">6</td><td class="s15"><a target="_blank" href="http://streetscore.media.mit.edu/static/files/streetscore_paper.pdf">StreetScore: Predicting the Perceived safety of one million streetscapes; Naik</a></td><td class="s16" dir="ltr">2014</td><td class="s17" colspan="6">GIST, SIFT, HOG</td><td class="s17" colspan="2">Ranking SVM</td><td class="s17">None</td><td class="s17">None</td><td class="s18">PlacePulse 1.0</td><td class="s19" colspan="4">Creates StreetScore Dataset</td></tr><tr style='height:16px;'><th id="254117349R124" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">125</div></th><td class="s5">7</td><td class="s15" dir="ltr"><a target="_blank" href="https://pdfs.semanticscholar.org/8322/916b759cd8f9a3cba584eecd52ff91ec94f9.pdf">Does the Visibility of Greenery Increase Perceived Safety in Urban Areas? Evidence from the Place Pulse 1.0 Dataset; Li</a></td><td class="s16">2015</td><td class="s17" colspan="6">AlexNet</td><td class="s17" colspan="2">SVM</td><td class="s17">SVM</td><td class="s17">70</td><td class="s18">PlacePulse 1.0</td><td class="s19" dir="ltr" colspan="4">CNN and SVM to rank and classify pereception scores</td></tr><tr style='height:16px;'><th id="254117349R125" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">126</div></th><td class="s5">8</td><td class="s4"><a target="_blank" href="http://senseable.mit.edu/treepedia/treepedia_publication.pdf">Assessing street-level urban greenery using Google Street View and a modified green view index; Li</a></td><td class="s14">2015</td><td class="s14" dir="ltr" colspan="6">ArcGis, threshold</td><td class="s14" colspan="2">Ponderado de areas</td><td class="s14"></td><td class="s14"></td><td class="s3" dir="ltr">Places205, GSV</td><td class="s3" dir="ltr" colspan="4">Creates the base to develop MIT TreePedia (green areas)</td></tr><tr style='height:16px;'><th id="254117349R126" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">127</div></th><td class="s5">9</td><td class="s15" dir="ltr"><a target="_blank" href="https://d1wqtxts1xzle7.cloudfront.net/40131536/fp026en-porzi.pdf?1447838606=&amp;response-content-disposition=inline%3B+filename%3DPredicting_and_Understanding_Urban_Perce.pdf&amp;Expires=1601960499&amp;Signature=O8lP6zhD1A-7RAD~t0mpGYfPzWgtkn4VrEgG1FaOA7ESC8D9szn7ow043WZd3-IzndFICw4abq72xpFT05UOF-ZJdew9h-L5JxhWu47BsOMRzjtjhc9fxAUsskxfTmx05mTBYtrdpc14skI9ciiNjLW5jFlrsKK5wf-6yFw1uyRuNlxNEuXAvy0PDOl83ytREYp~1vldff0-NAkrBAUCilSRzRDlf~a3XSZN36DuLrbeob29HOJ~HNXAnU6GRWwMDU7Ca72-itQ-vvl~0ta-CY8gHdq5pj9d57KPMRmCAHmRu320omLDzQ0P9p~82hnuPN9uyfzK4physZiEwZ9I6A__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">Predicting and Understanding Urban Perception with Convolutional Neural Networks; Porzi</a></td><td class="s18" dir="ltr">2015</td><td class="s17" dir="ltr" colspan="6">Threshold, computer Vision, feature extractor gist</td><td class="s17" dir="ltr" colspan="2">Ranking SVM</td><td class="s17"></td><td class="s17"></td><td class="s18" dir="ltr">PlacePulse 1.0</td><td class="s19" dir="ltr" colspan="4">Examining Place Pulse and how green areas impact of the safety scores of the images</td></tr><tr style='height:16px;'><th id="254117349R127" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">128</div></th><td class="s5">10</td><td class="s15" dir="ltr"><a target="_blank" href="https://static1.squarespace.com/static/5759bc7886db431d658b7d33/t/5759bf4086db431d658b926a/1465499456692/NaikRaskarHidalgo_AER2016.pdf">Cities Are Physical Too: Using Computer Vision to Measure the Quality and Impact of Urban Appearance</a></td><td class="s18" dir="ltr">2016</td><td class="s19" dir="ltr" colspan="6">Texton histograms, CIELAB 3D color<br>histograms, and GIST</td><td class="s19" dir="ltr" colspan="2">SVM</td><td class="s19">SVM</td><td class="s19"></td><td class="s18">PlacePulse 1.0</td><td class="s19" colspan="4">Review about StreetScore</td></tr><tr style='height:16px;'><th id="254117349R128" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">129</div></th><td class="s5">11</td><td class="s15" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1608.01769.pdf">Deep Learning the City : Quantifying Urban Perception At A Global Scale; Dubey</a></td><td class="s16" dir="ltr">2016</td><td class="s17" colspan="6">AlexNet, VGGNet, PlaceNet</td><td class="s17" colspan="2">SVM/ RSS-CNN</td><td class="s17">SS-CNN</td><td class="s17">73</td><td class="s18">PlacePulse 2.0</td><td class="s19" dir="ltr" colspan="4">Creates Place Pulse 2.0</td></tr><tr style='height:16px;'><th id="254117349R129" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">130</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="http://senseable.mit.edu/papers/pdf/20170512_Seiferling_etal_GreenStreets_LandscapeUrbanPlanning.pdf">Green streets- Quantifying and mapping urban trees with street-level imagery and computer vision; Seiferling</a></td><td class="s14" dir="ltr">2017</td><td class="s14" dir="ltr" colspan="6">LiDAR</td><td class="s14" colspan="2"></td><td class="s14"></td><td class="s14"></td><td class="s3" dir="ltr">City Streetscapes</td><td class="s3" dir="ltr" colspan="4">Quantifying and mapping urban trees, creates Treepedia Maps</td></tr><tr style='height:16px;'><th id="254117349R130" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">131</div></th><td class="s5">13</td><td class="s4" dir="ltr"><a target="_blank" href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.170170">Using Deep Learning to Quantify the Beauty of Outdoor Places; Seresinhe</a></td><td class="s14" dir="ltr">2017</td><td class="s14" dir="ltr" colspan="6">VGG, ALexNet, GoogleNet, ResNet, PlaceNet</td><td class="s14" colspan="2"></td><td class="s14" dir="ltr">CNN</td><td class="s14" dir="ltr">65</td><td class="s3" dir="ltr">SUN, Places365</td><td class="s3" dir="ltr" colspan="4">Identify visual components to detect which feature can define concepts like beauty</td></tr><tr style='height:16px;'><th id="254117349R131" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">132</div></th><td class="s5">14</td><td class="s15"><a target="_blank" href="https://web.cs.ucla.edu/~yuanluxu/publications/event_reason_acmmm17.pdf">Place-centric Visual Urban Perception with Deep Multi-instance<br>Regression; Liu</a></td><td class="s16">2017</td><td class="s17" colspan="6">Instance Level, Multi Instance Learning</td><td class="s19" dir="ltr" colspan="2">Multi-instance Regression, SVM</td><td class="s17">HDMiR</td><td class="s17">83</td><td class="s18" dir="ltr">Place-Centric, Place Pulse 1.0, 2.0</td><td class="s19" colspan="4">Creates Place-Centric dataset based on crimes</td></tr><tr style='height:16px;'><th id="254117349R132" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">133</div></th><td class="s5">15</td><td class="s4"><a target="_blank" href="http://www.nvc.cs.vt.edu/~ctlu/Publication/2018/ACM-GIS-2018-Kaiqun.pdf">StreetNet: Preference Learning with Convolutional Neural<br>Network on Urban Crime Perception; Fu</a></td><td class="s14">2018</td><td class="s14" colspan="6">VGGNet, Preference Learning</td><td class="s14" colspan="2">SVM, StreetNet</td><td class="s14"></td><td class="s14"></td><td class="s3">DC-k, NYC-k</td><td class="s3" colspan="4">Creates DC-K and D-k datasets (crimes)</td></tr><tr style='height:16px;'><th id="254117349R133" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">134</div></th><td class="s5">16</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1902.06871.pdf">Predicting city safety perception based on visual image content; Acosta</a></td><td class="s14">2018</td><td class="s14" colspan="6">VGG19</td><td class="s14" colspan="2">Fine-Tuning</td><td class="s14">Softmax</td><td class="s14"></td><td class="s3"></td><td class="s3" colspan="4">Creates their dataset about Streets of Bogota</td></tr><tr style='height:16px;'><th id="254117349R134" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">135</div></th><td class="s5">17</td><td class="s20" dir="ltr"><a target="_blank" href="http://bzhou.ie.cuhk.edu.hk/publication/landscape_urbanplanning.pdf">Measuring human perceptions of a large-scale urban region using machine learning; Zhang</a></td><td class="s16">2018</td><td class="s17" dir="ltr" colspan="6">ResNet, semantic map, perception map</td><td class="s19" dir="ltr" colspan="2">Multi-Linear Regression, SVM</td><td class="s17">SS-CNN</td><td class="s17">68</td><td class="s18">PlacePulse 2.0</td><td class="s19" dir="ltr" colspan="4">Multi-linear regression between percep scores and visual components segmentations</td></tr><tr style='height:16px;'><th id="254117349R135" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">136</div></th><td class="s21">18</td><td class="s15" dir="ltr"><a target="_blank" href="https://www.idiap.ch/~gatica/publications/SantaniRuizGatica-tsc18.pdf">Looking South: Learning Urban Perception in Developing Cities</a></td><td class="s18" dir="ltr">2018</td><td class="s22" dir="ltr" colspan="6"></td><td class="s19" colspan="2"></td><td class="s17"></td><td class="s17"></td><td class="s18">PlacePulse 2.0</td><td class="s19" colspan="4"></td></tr><tr style='height:16px;'><th id="254117349R136" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">137</div></th><td class="s5">19</td><td class="s22" dir="ltr">Multi-Task Deep Relative Attribute Learning for Visual Urban Perception; Min</td><td class="s16" dir="ltr">2019</td><td class="s17" dir="ltr" colspan="6"></td><td class="s19" colspan="2"></td><td class="s17"></td><td class="s17"></td><td class="s18" dir="ltr">Place Pulse 1.0, 2.0</td><td class="s19" colspan="4"></td></tr><tr style='height:16px;'><th id="254117349R137" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">138</div></th><td class="s5">20</td><td class="s22" dir="ltr">Visual Urban Perception with Deep Semantic-Aware Network; Xu</td><td class="s16" dir="ltr">2019</td><td class="s17" dir="ltr" colspan="6"></td><td class="s19" colspan="2"></td><td class="s17"></td><td class="s17"></td><td class="s18">PlacePulse 2.0</td><td class="s19" colspan="4"></td></tr><tr style='height:16px;'><th id="254117349R138" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">139</div></th><td class="s5">21</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1904.04336.pdf">Quantifying the Presence of Graffiti in Urban Environments; Tokuda</a></td><td class="s14" dir="ltr">2019</td><td class="s14" dir="ltr" colspan="6">ResNet, Mask R-CNN, YOLOv3</td><td class="s14" colspan="2">Ponderado de areas</td><td class="s14"></td><td class="s14"></td><td class="s3">COCO</td><td class="s3" colspan="4">Determine graffiti level index in Sao Paulo city</td></tr><tr style='height:16px;'><th id="254117349R139" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">140</div></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R140" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">141</div></th><td></td><td></td><td></td><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R141" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">142</div></th><td></td><td></td><td></td><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R142" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">143</div></th><td></td><td></td><td></td><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R143" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">144</div></th><td></td><td></td><td></td><td></td><td></td><td class="s12" dir="ltr"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R144" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">145</div></th><td></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s13" dir="ltr"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td></td></tr><tr style='height:16px;'><th id="254117349R145" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">146</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="16" rowspan="2">Machine Learning  Interpretability</td><td></td></tr><tr style='height:16px;'><th id="254117349R146" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">147</div></th><td class="s1"></td><td></td></tr><tr style='height:16px;'><th id="254117349R147" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">148</div></th><td class="s1"></td><td class="s3" dir="ltr" rowspan="2">Paper</td><td class="s3" rowspan="2">Año</td><td class="s3" dir="ltr" colspan="6" rowspan="2">Goal</td><td class="s3" dir="ltr" colspan="3" rowspan="2">Enfoque</td><td class="s3" dir="ltr" rowspan="2">Alcance</td><td class="s3" dir="ltr" rowspan="2">Model type</td><td class="s3" colspan="3" rowspan="2">Contribution</td><td></td></tr><tr style='height:16px;'><th id="254117349R148" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">149</div></th><td class="s1"></td><td></td></tr><tr style='height:16px;'><th id="254117349R149" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">150</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1602.04938.pdf">“Why Should I Trust You?”<br>Explaining the Predictions of Any Classifier</a></td><td class="s3" dir="ltr">2016</td><td class="s3" dir="ltr" colspan="6">Based on the prediction, locally generates random values close to evaluate the faithful of those values and ratify the prediction</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Local</td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">LIME: Local Explanation</td><td></td></tr><tr style='height:16px;'><th id="254117349R150" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">151</div></th><td class="s5">2</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1606.05386v1.pdf">Model-Agnostic Interpretability of Machine Learning</a></td><td class="s3" dir="ltr">2016</td><td class="s3" dir="ltr" colspan="6">Brief review of LIME</td><td class="s3" dir="ltr" colspan="3">Discuss</td><td class="s3"></td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">Discussion and review of LIME results</td><td></td></tr><tr style='height:16px;'><th id="254117349R151" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">152</div></th><td class="s5">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1706.07160.pdf">MAGIX: Model Agnostic Globally Interpretable Explanations</a></td><td class="s3" dir="ltr">2017</td><td class="s3" dir="ltr" colspan="6">Linear models Interpretability, learning process by conditionals or set of conditionals (iris dataset, wisconsin breast cancer dataset, car evaluation dataset)</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Global</td><td class="s3" dir="ltr">Black-box Linear Classifiers</td><td class="s3" dir="ltr" colspan="3">MAGIX: Global Method to interprete linear classification models</td><td></td></tr><tr style='height:16px;'><th id="254117349R152" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">153</div></th><td class="s5">4</td><td class="s3" dir="ltr">Montavon</td><td class="s3" dir="ltr">2017</td><td class="s3" dir="ltr" colspan="6">Using the LRP in DNN can extract and highlights object using heatmaps</td><td class="s3" dir="ltr" colspan="3">Model-level</td><td class="s3" dir="ltr">Local</td><td class="s3" dir="ltr">Black-box DNN models</td><td class="s3" dir="ltr" colspan="3">Creates the Layer-wise Relevance Propagation (LRP) framework for explanation</td><td></td></tr><tr style='height:16px;'><th id="254117349R153" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">154</div></th><td class="s5">5</td><td class="s3" dir="ltr">Tsang</td><td class="s3" dir="ltr">2017</td><td class="s3" dir="ltr" colspan="6">Identify the changing state of weights in MLP, </td><td class="s3" dir="ltr" colspan="3">Model-level</td><td class="s3" dir="ltr"></td><td class="s3" dir="ltr">NN</td><td class="s3" dir="ltr" colspan="3">NID: Framework to detect the statical interaction between networks</td><td></td></tr><tr style='height:16px;'><th id="254117349R154" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">155</div></th><td class="s5">6</td><td class="s3" dir="ltr">Tsang</td><td class="s3" dir="ltr">2018</td><td class="s3" dir="ltr" colspan="6">Using neural interaction detection (NID) framework (proposed by them) and them growth using Hierarchical Interactions</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Local -&gt; Global</td><td class="s3" dir="ltr">Black-box DNN, RNN models</td><td class="s3" dir="ltr" colspan="3">Propose Mahe: a framework to explain the context-dependent and context-free structures of any model</td><td></td></tr><tr style='height:16px;'><th id="254117349R155" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">156</div></th><td class="s5">7</td><td class="s3" dir="ltr">Gilpin</td><td class="s3" dir="ltr">2018</td><td class="s3" dir="ltr" colspan="6">Review a lot of works proposed until that time</td><td class="s3" dir="ltr" colspan="3">Theory</td><td class="s3"></td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">Divide the explanation process in three: Processing, Representation and explanation.</td><td></td></tr><tr style='height:16px;'><th id="254117349R156" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">157</div></th><td class="s5">8</td><td class="s3" dir="ltr">Murdoch</td><td class="s3" dir="ltr">2019</td><td class="s3" dir="ltr" colspan="6">predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience</td><td class="s3" dir="ltr" colspan="3">Dataset-level y Prediction-Level</td><td class="s3"></td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">Creates the <span style="font-weight:bold;">P</span>redictive, <span style="font-weight:bold;">D</span>escriptive, <span style="font-weight:bold;">R</span>elevant (PDR) framework to discuss interpretations.</td><td></td></tr><tr style='height:16px;'><th id="254117349R157" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">158</div></th><td class="s5">9</td><td class="s4" dir="ltr"><a target="_blank" href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors: High-Precision Model-Agnostic Explanations</a></td><td class="s3" dir="ltr">2018</td><td class="s3" dir="ltr" colspan="6">Based on Conditional predictions, Anchors propose a Probability distribution closely to the prediction and depending on what is the probability to get a better result the original chose the answer</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Global</td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">Anchors: Global Explanation</td><td></td></tr><tr style='height:16px;'><th id="254117349R158" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">159</div></th><td class="s5">10</td><td class="s3" dir="ltr">Plumb</td><td class="s3" dir="ltr">2018</td><td class="s3" dir="ltr" colspan="6">for local they use a SVM or K-Nearest model to determine the best explanation for prediction.</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Local</td><td class="s3" dir="ltr">Black-box</td><td class="s3" dir="ltr" colspan="3">MAPLE: Local Explanation</td><td></td></tr><tr style='height:16px;'><th id="254117349R159" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">160</div></th><td class="s5">11</td><td class="s3" dir="ltr">Carter</td><td class="s3" dir="ltr">2018</td><td class="s3" dir="ltr" colspan="6">SIS (Sufficient Input Subset)</td><td class="s3" dir="ltr" colspan="3">Prediction-level</td><td class="s3" dir="ltr">Local-Global</td><td class="s3" dir="ltr">Black-Box</td><td class="s3" dir="ltr" colspan="3">A simple method to understand black box models based on SIS</td><td></td></tr><tr style='height:16px;'><th id="254117349R160" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">161</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1908.00087.pdf">explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning</a></td><td class="s3" dir="ltr">2019</td><td class="s3" dir="ltr" colspan="6">Merge of a lot of explainer and get the common explanation from them</td><td class="s3" dir="ltr" colspan="3">Prediction and model level</td><td class="s3" dir="ltr">Local-Global</td><td class="s3" dir="ltr">Black-Box</td><td class="s3" dir="ltr" colspan="3">Framework to visualize explainations of black box models</td><td></td></tr><tr style='height:16px;'><th id="254117349R161" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">162</div></th><td></td><td class="s23"></td><td></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td></td></tr><tr style='height:16px;'><th id="254117349R162" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">163</div></th><td></td><td class="s23"></td><td></td><td class="s23"></td><td class="s23"></td><td></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td></td></tr><tr style='height:16px;'><th id="254117349R163" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">164</div></th><td></td><td class="s23" dir="ltr"></td><td></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td></td></tr><tr style='height:16px;'><th id="254117349R164" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">165</div></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td class="s23"></td><td></td></tr><tr style='height:16px;'><th id="254117349R165" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">166</div></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R166" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">167</div></th><td></td><td class="s0"></td><td class="s0"></td><td class="s0"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R167" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">168</div></th><td class="s1"></td><td class="s2" dir="ltr" colspan="3" rowspan="2">CNN Features Visualizers</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R168" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">169</div></th><td class="s1"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R169" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">170</div></th><td class="s1"></td><td class="s3" dir="ltr" rowspan="2">Paper</td><td class="s3" dir="ltr" rowspan="2">Nombre</td><td class="s3" rowspan="2">Año</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R170" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">171</div></th><td class="s1"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R171" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">172</div></th><td class="s5" dir="ltr">1</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1311.2901.pdf">Visualizing and Understanding Convolutional Networks</a></td><td class="s3" dir="ltr">Convolutions</td><td class="s3" dir="ltr">2014</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R172" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">173</div></th><td class="s5">2</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1312.6034.pdf">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a></td><td class="s3" dir="ltr">Saliency Maps</td><td class="s3" dir="ltr">2014</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R173" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">174</div></th><td class="s5">3</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1412.6806.pdf">Striving For Simplicity: The All Convolutional Networks</a></td><td class="s3" dir="ltr">Guidad Back Propagation</td><td class="s3" dir="ltr">2015</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R174" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">175</div></th><td class="s5">4</td><td class="s4" dir="ltr"><a target="_blank" href="https://pdfs.semanticscholar.org/17a2/73bbd4448083b01b5a9389b3c37f5425aac0.pdf">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</a></td><td class="s3" dir="ltr">LRP</td><td class="s3" dir="ltr">2015</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R175" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">176</div></th><td class="s5">5</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1512.02479.pdf">Explaining NonLinear Classification Decisions with Deep Taylor Decomposition</a></td><td class="s3" dir="ltr">DeepTaylor</td><td class="s3" dir="ltr">2015</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R176" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">177</div></th><td class="s5">6</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1605.01713.pdf">Not Just A Black Box: Learning Important Features Through Propagating Activation Differences</a></td><td class="s3" dir="ltr">DeepLIFT</td><td class="s3" dir="ltr">2016</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R177" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">178</div></th><td class="s5">7</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1512.04150.pdf">CAM: Learning Deep Features for Discriminative Localization</a></td><td class="s3" dir="ltr">CAM</td><td class="s3" dir="ltr">2016</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R178" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">179</div></th><td class="s1"></td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1702.04595.pdf">VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS</a></td><td class="s3" dir="ltr">Visualize</td><td class="s3" dir="ltr">2017</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R179" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">180</div></th><td class="s5">8</td><td class="s4" dir="ltr"><a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a></td><td class="s3" dir="ltr">grad-CAM</td><td class="s3" dir="ltr">2017</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R180" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">181</div></th><td class="s5">9</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1706.03825.pdf">SmoothGrad: removing noise by adding noise</a></td><td class="s3" dir="ltr">SmoothGrad</td><td class="s3" dir="ltr">2017</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R181" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">182</div></th><td class="s5">10</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1703.01365.pdf">Axiomatic Attribution for Deep Networks</a></td><td class="s3" dir="ltr">Integrated Grad</td><td class="s3" dir="ltr">2017</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R182" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">183</div></th><td class="s5">11</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1710.11063.pdf">Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks</a></td><td class="s3" dir="ltr">Class Activation Maps</td><td class="s3" dir="ltr">2018</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R183" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">184</div></th><td class="s5">12</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1810.03292.pdf">Sanity Checks for Saliency Maps</a></td><td class="s3" dir="ltr">Saliency Maps Summary</td><td class="s3" dir="ltr">2018</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R184" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">185</div></th><td class="s5">13</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1704.02685.pdf">Learning Important Features Through Propagating Activation Differences</a></td><td class="s3" dir="ltr">DeepLIFT</td><td class="s3" dir="ltr">2019</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R185" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">186</div></th><td class="s5">14</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1902.02497.pdf">CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks</a></td><td class="s3" dir="ltr">CHIP</td><td class="s3" dir="ltr">2019</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R186" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">187</div></th><td class="s5">15</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1806.00069.pdf">Explaining Explanations: An Overview of Interpretability of Machine Learning</a></td><td class="s3" dir="ltr">Explainers</td><td class="s3" dir="ltr">2019</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R187" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">188</div></th><td class="s5">16</td><td class="s4" dir="ltr"><a target="_blank" href="https://zpascal.net/cvpr2019/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.pdf">Attention Branch Network: Learning of Attention Mechanism for Visual Explanation</a></td><td class="s3" dir="ltr">Attention Branch</td><td class="s3" dir="ltr">2019</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R188" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">189</div></th><td class="s5">17</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/1909.06342.pdf">Explainable Machine Learning in Deployment</a></td><td class="s3" dir="ltr"></td><td class="s3" dir="ltr">2019</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr style='height:16px;'><th id="254117349R189" style="height: 16px;" class="row-headers-background"><div class="row-header-wrapper" style="line-height: 16px;">190</div></th><td class="s5">18</td><td class="s4" dir="ltr"><a target="_blank" href="https://arxiv.org/pdf/2004.14545.pdf">Explainable Deep Learning: A Field Guide for the Uninitiated</a></td><td class="s3" dir="ltr"></td><td class="s3" dir="ltr">2020</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>
<script type='text/javascript' nonce="AEKa39iBCPP5TL44lykBKw">
function posObj(sheet, id, row, col, x, y) {
  var rtl = false;
  var sheetElement = document.getElementById(sheet);
  if (!sheetElement) {
    sheetElement = document.getElementById(sheet + '-grid-container');
  }
  if (sheetElement) {
    rtl = sheetElement.getAttribute('dir') == 'rtl';
  }
  var r = document.getElementById(sheet+'R'+row);
  var c = document.getElementById(sheet+'C'+col);
  if (r && c) {
    var objElement = document.getElementById(id);
    var s = objElement.style;
    var t = y;
    while (r && r != sheetElement) {
      t += r.offsetTop;
      r = r.offsetParent;
    }
    var offsetX = x;
    while (c && c != sheetElement) {
      offsetX += c.offsetLeft;
      c = c.offsetParent;
    }
    if (rtl) {
      offsetX -= objElement.offsetWidth;
    }
    s.left = offsetX + 'px';
    s.top = t + 'px';
    s.display = 'block';
    s.border = '1px solid #000000';
  }
};
function posObjs() {
};
posObjs();</script>
