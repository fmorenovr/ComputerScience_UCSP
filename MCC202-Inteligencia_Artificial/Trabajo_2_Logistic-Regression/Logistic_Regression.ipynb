{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and upload all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import libraries and preprocess texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('data/shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jenazads/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop = stopwords.words('english') # Common words\n",
    "porter = PorterStemmer() # Getting root of words\n",
    "char3=stop[:17] # Getting 1st and 2nd person pronouns\n",
    "stop=stop[17:116]+stop[118:] \n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', ':)', ':)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stream=stream_docs(path='data/shuffled_movie_data.csv')\n",
    "docs, y = [], []\n",
    "for _ in range(50000):\n",
    "    text_aux, label =next(doc_stream)\n",
    "    text=tokenizer(text_aux)\n",
    "    docs.append(text)\n",
    "    y.append(label)\n",
    "    #print('\\n',tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing trash ad duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeDuplicates(listofElements):\n",
    "    # Create an empty list to store unique elements\n",
    "    noDupl = []\n",
    "    # Iterate over the original list and for each element\n",
    "    # add it to uniqueList, if its not already there.\n",
    "    for elem in listofElements:\n",
    "        if elem not in noDupl:\n",
    "            noDupl.append(elem)\n",
    "    \n",
    "    # Return the list of unique elements        \n",
    "    return noDupl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasifying positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# texto_dividido: a text splitted as a List of Words\n",
    "# posneg_dict: Positive and negative dictionary\n",
    "\n",
    "# Output:\n",
    "# COUNT_POSITIVE: # Of positive words according to the dictionary\n",
    "# COUNT_NEGATIVE: # Of negative words according to the dictionary\n",
    "def getPositiveNegativeCountWords(texto_dividido, posneg_dictionary):\n",
    "        # Count the positive words\n",
    "    COUNT_POSITIVE = 0\n",
    "    COUNT_NEGATIVE = 0\n",
    "    for word in texto_dividido:\n",
    "        try:\n",
    "            val = posneg_dictionary[word]\n",
    "            if val == 1:\n",
    "                COUNT_POSITIVE = COUNT_POSITIVE + 1\n",
    "            elif val == 0:\n",
    "                COUNT_NEGATIVE = COUNT_NEGATIVE + 1\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return (float(COUNT_POSITIVE), float(COUNT_NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2041\n",
      "1417\n",
      "4818\n",
      "3186\n"
     ]
    }
   ],
   "source": [
    "positives=[line.strip() for line in open('data/positive-words.txt')]\n",
    "positives = [porter.stem(w) for w in positives]\n",
    "print(len(positives))\n",
    "positives=removeDuplicates(positives)\n",
    "print(len(positives))\n",
    "negatives=[line.strip() for line in open('data/negative-words.txt')]\n",
    "negatives = [porter.stem(w) for w in negatives]\n",
    "print(len(negatives))\n",
    "negatives=removeDuplicates(negatives)\n",
    "print(len(negatives))\n",
    "pron12=stop[:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all words in data set from positive.csv and negative.csv, then will be compare with words in opinion english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['portray', 'famou', 'memor', 'earlier', 'recommend', 'oscar', 'anim', 'creat', 'member', 'touch', 'meet', 'battl', 'fantasi', 'anim', 'today', 'set', 'disney', 'older', 'journey', '7', 'emot', 'tale', 'season', 'era', 'emot', 'talent', 'creepi', 'escap', 'complex', 'plenti', 'perfectli', 'see', 'romant', 'bring', 'natur', 'recent', 'deserv', 'charm', 'intellig', 'brother', 'subtl', 'younger', 'stage', 'america', 'polit', 'portray', 'move', 'mark', 'bond', 'creat', 'master', 'enjoy', 'move', 'cold', 'truth', 'situat', 'societi', 'fill', 'unlik', 'follow', 'stun', 'bill', 'thank', 'edg', 'italian', 'insid', 'appreci', 'awesom', 'lead', 'clever', 'fantast', 'power', 'beauti', 'social', 'pace', 'it!', '70', 'solid', 'adventur', 'tell', 'danc', 'dream', '!', 'success', 'manag', 'impress', 'atmospher', 'offic', 'dramat', 'outstand', 'greatest', 'begin', 'brilliant', 'use', 'cultur', 'scott', 'keep', 'incred', 'featur', 'wonder']\n",
      "['channel', 'need', 'store', 'aw', 'bunch', 'save', 'wors', 'yeah', 'weird', 'apart', 'lee', 'cop', 'reason', 'quickli', 'dumb', 'miss', 'badli', 'end', 'bother', 'garbag', 'unless', 'imdb', 'sorri', 'remak', 'premis', 'wast', 'credit', 'excus', 'fire', 'annoy', 'incred', 'pain', 'parti', 'wait', 'dialog', 'van', 'writer', 'horribl', 'predict', 'slasher', 'begin', 'cast', 'sequel', 'forward', 'okay', 'pathet', 'write', 'lame', 'neither', 'accent', 'werent', '30', 'walk', 'trash', 'speak', 'spend', 'pay', 'dull', 'pointless', 'spent', 'alien', 'fake', 'posit', 'avoid', 'zombi', 'kill', 'consid', 'weak', 'unbeliev', 'project', 'island', 'twist', 'nuditi', '20', 'cover', 'monster', 'direct', 'cheesi', 'develop', 'produc', 'brain', 'gay', 'plain', 'worst', 'stupid', 'possibl', 'stick', 'meant', 'potenti', 'fail', 'valu', 'produc', 'girlfriend', 'bore', 'total', 'hair', 'bare', 'whatev', 'ok', 'laugh']\n"
     ]
    }
   ],
   "source": [
    "P2=100 # Number of features\n",
    "\n",
    "#We separate common words\n",
    "positive2 = pd.read_csv('data/positive.csv', index_col=0)\n",
    "positive2_i = positive2.index.values[:1000]\n",
    "stop2 = stopwords.words('english')\n",
    "positive2_i=set(positive2_i).difference(stop2)\n",
    "\n",
    "\n",
    "negative2 = pd.read_csv('data/negative.csv', index_col=0)\n",
    "negative2_i = negative2.index.values[:1000]\n",
    "negative2_i=set(negative2_i).difference(stop2)\n",
    "\n",
    "# We proced to delete repeated words.\n",
    "positive2=set(positive2_i).difference(negative2_i)\n",
    "negative2=set(negative2_i).difference(positive2_i)\n",
    "positive2=list(positive2)[:P2]\n",
    "negative2=list(negative2)[:P2]\n",
    "\n",
    "positive2 = [porter.stem(w) for w in positive2]\n",
    "negative2 = [porter.stem(w) for w in negative2]\n",
    "\n",
    "print(positive2)\n",
    "print(negative2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "(50000, 6)\n"
     ]
    }
   ],
   "source": [
    "N=50000\n",
    "\n",
    "X=np.zeros((N,6))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    if idx%1000==0:\n",
    "        print(idx)\n",
    "    X[idx,5]+=np.log(len(com)) #len: char 6\n",
    "    for word in com:\n",
    "        X[idx,3]+=char3.count(word) #pronoun: char 4\n",
    "        if word=='!':\n",
    "            X[idx,4]=1 #! simbol: char 5\n",
    "        if (word=='no' or word=='not'):\n",
    "            X[idx,2]=1 #! simbol: char 5\n",
    "        X[idx,0]+=positives.count(word) #positive words : char 1\n",
    "        X[idx,1]+=negatives.count(word) #negative words : char 2\n",
    "    idx+=1\n",
    "\n",
    "X2=np.zeros((N,2*P2))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    #if com.count('terrific'):\n",
    "        #print('foundddddd')\n",
    "    #if idx%1000==0:\n",
    "    #    print(idx)\n",
    "    idx2=0\n",
    "    for word in positive2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    #print(idx2)\n",
    "    for word in negative2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    idx+=1\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to combine all data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=np.concatenate((X, X2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.asarray(y)\n",
    "y=y.reshape(y.shape[0],1)\n",
    "x_train=X_data[:40000]\n",
    "x_valid=X_data[40000:45000]\n",
    "x_test=X_data[45000:50000]\n",
    "y_train=y[:40000]\n",
    "y_valid=y[40000:45000]\n",
    "y_test=y[45000:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca:  0\n",
      "--------------\n",
      "Precision:  [49.1]\n",
      "Epoca:  1000\n",
      "--------------\n",
      "Precision:  [66.96]\n",
      "Epoca:  2000\n",
      "--------------\n",
      "Precision:  [67.24]\n",
      "Epoca:  3000\n",
      "--------------\n",
      "Precision:  [67.6]\n",
      "Epoca:  4000\n",
      "--------------\n",
      "Precision:  [67.94]\n",
      "Epoca:  5000\n",
      "--------------\n",
      "Precision:  [68.12]\n",
      "Epoca:  6000\n",
      "--------------\n",
      "Precision:  [68.52]\n",
      "Epoca:  7000\n",
      "--------------\n",
      "Precision:  [68.66]\n",
      "Epoca:  8000\n",
      "--------------\n",
      "Precision:  [69.]\n",
      "Epoca:  9000\n",
      "--------------\n",
      "Precision:  [69.22]\n",
      "Epoca:  10000\n",
      "--------------\n",
      "Precision:  [69.38]\n",
      "Epoca:  11000\n",
      "--------------\n",
      "Precision:  [69.6]\n",
      "Epoca:  12000\n",
      "--------------\n",
      "Precision:  [69.7]\n",
      "Epoca:  13000\n",
      "--------------\n",
      "Precision:  [69.82]\n",
      "Epoca:  14000\n",
      "--------------\n",
      "Precision:  [70.06]\n",
      "Epoca:  15000\n",
      "--------------\n",
      "Precision:  [70.16]\n",
      "Epoca:  16000\n",
      "--------------\n",
      "Precision:  [70.2]\n",
      "Epoca:  17000\n",
      "--------------\n",
      "Precision:  [70.14]\n",
      "Epoca:  18000\n",
      "--------------\n",
      "Precision:  [70.24]\n",
      "Epoca:  19000\n",
      "--------------\n",
      "Precision:  [70.46]\n",
      "Epoca:  20000\n",
      "--------------\n",
      "Precision:  [70.58]\n",
      "Epoca:  21000\n",
      "--------------\n",
      "Precision:  [70.56]\n",
      "Epoca:  22000\n",
      "--------------\n",
      "Precision:  [70.56]\n",
      "Epoca:  23000\n",
      "--------------\n",
      "Precision:  [70.56]\n",
      "Epoca:  24000\n",
      "--------------\n",
      "Precision:  [70.5]\n",
      "Epoca:  25000\n",
      "--------------\n",
      "Precision:  [70.52]\n",
      "Epoca:  26000\n",
      "--------------\n",
      "Precision:  [70.52]\n",
      "Epoca:  27000\n",
      "--------------\n",
      "Precision:  [70.64]\n",
      "Epoca:  28000\n",
      "--------------\n",
      "Precision:  [70.9]\n",
      "Epoca:  29000\n",
      "--------------\n",
      "Precision:  [70.98]\n",
      "Epoca:  30000\n",
      "--------------\n",
      "Precision:  [71.08]\n",
      "Epoca:  31000\n",
      "--------------\n",
      "Precision:  [71.18]\n",
      "Epoca:  32000\n",
      "--------------\n",
      "Precision:  [71.28]\n",
      "Epoca:  33000\n",
      "--------------\n",
      "Precision:  [71.28]\n",
      "Epoca:  34000\n",
      "--------------\n",
      "Precision:  [71.4]\n",
      "Epoca:  35000\n",
      "--------------\n",
      "Precision:  [71.42]\n",
      "Epoca:  36000\n",
      "--------------\n",
      "Precision:  [71.42]\n",
      "Epoca:  37000\n",
      "--------------\n",
      "Precision:  [71.48]\n",
      "Epoca:  38000\n",
      "--------------\n",
      "Precision:  [71.64]\n",
      "Epoca:  39000\n",
      "--------------\n",
      "Precision:  [71.64]\n",
      "Epoca:  40000\n",
      "--------------\n",
      "Precision:  [71.6]\n",
      "Epoca:  41000\n",
      "--------------\n",
      "Precision:  [71.76]\n",
      "Epoca:  42000\n",
      "--------------\n",
      "Precision:  [71.94]\n",
      "Epoca:  43000\n",
      "--------------\n",
      "Precision:  [72.]\n",
      "Epoca:  44000\n",
      "--------------\n",
      "Precision:  [72.12]\n",
      "Epoca:  45000\n",
      "--------------\n",
      "Precision:  [72.24]\n",
      "Epoca:  46000\n",
      "--------------\n",
      "Precision:  [72.3]\n",
      "Epoca:  47000\n",
      "--------------\n",
      "Precision:  [72.42]\n",
      "Epoca:  48000\n",
      "--------------\n",
      "Precision:  [72.54]\n",
      "Epoca:  49000\n",
      "--------------\n",
      "Precision:  [72.52]\n",
      "Epoca:  50000\n",
      "--------------\n",
      "Precision:  [72.64]\n",
      "Epoca:  51000\n",
      "--------------\n",
      "Precision:  [72.72]\n",
      "Epoca:  52000\n",
      "--------------\n",
      "Precision:  [72.74]\n",
      "Epoca:  53000\n",
      "--------------\n",
      "Precision:  [72.82]\n",
      "Epoca:  54000\n",
      "--------------\n",
      "Precision:  [72.84]\n",
      "Epoca:  55000\n",
      "--------------\n",
      "Precision:  [72.94]\n",
      "Epoca:  56000\n",
      "--------------\n",
      "Precision:  [73.]\n",
      "Epoca:  57000\n",
      "--------------\n",
      "Precision:  [73.08]\n",
      "Epoca:  58000\n",
      "--------------\n",
      "Precision:  [73.14]\n",
      "Epoca:  59000\n",
      "--------------\n",
      "Precision:  [73.16]\n",
      "Epoca:  60000\n",
      "--------------\n",
      "Precision:  [73.14]\n",
      "Epoca:  61000\n",
      "--------------\n",
      "Precision:  [73.1]\n",
      "Epoca:  62000\n",
      "--------------\n",
      "Precision:  [73.2]\n",
      "Epoca:  63000\n",
      "--------------\n",
      "Precision:  [73.28]\n",
      "Epoca:  64000\n",
      "--------------\n",
      "Precision:  [73.26]\n",
      "Epoca:  65000\n",
      "--------------\n",
      "Precision:  [73.32]\n",
      "Epoca:  66000\n",
      "--------------\n",
      "Precision:  [73.34]\n",
      "Epoca:  67000\n",
      "--------------\n",
      "Precision:  [73.34]\n",
      "Epoca:  68000\n",
      "--------------\n",
      "Precision:  [73.44]\n",
      "Epoca:  69000\n",
      "--------------\n",
      "Precision:  [73.5]\n",
      "Epoca:  70000\n",
      "--------------\n",
      "Precision:  [73.58]\n",
      "Epoca:  71000\n",
      "--------------\n",
      "Precision:  [73.62]\n",
      "Epoca:  72000\n",
      "--------------\n",
      "Precision:  [73.68]\n",
      "Epoca:  73000\n",
      "--------------\n",
      "Precision:  [73.68]\n",
      "Epoca:  74000\n",
      "--------------\n",
      "Precision:  [73.7]\n",
      "Epoca:  75000\n",
      "--------------\n",
      "Precision:  [73.68]\n",
      "Epoca:  76000\n",
      "--------------\n",
      "Precision:  [73.66]\n",
      "Epoca:  77000\n",
      "--------------\n",
      "Precision:  [73.68]\n",
      "Epoca:  78000\n",
      "--------------\n",
      "Precision:  [73.72]\n",
      "Epoca:  79000\n",
      "--------------\n",
      "Precision:  [73.76]\n",
      "Epoca:  80000\n",
      "--------------\n",
      "Precision:  [73.74]\n",
      "Epoca:  81000\n",
      "--------------\n",
      "Precision:  [73.76]\n",
      "Epoca:  82000\n",
      "--------------\n",
      "Precision:  [73.76]\n",
      "Epoca:  83000\n",
      "--------------\n",
      "Precision:  [73.76]\n",
      "Epoca:  84000\n",
      "--------------\n",
      "Precision:  [73.78]\n",
      "Epoca:  85000\n",
      "--------------\n",
      "Precision:  [73.84]\n",
      "Epoca:  86000\n",
      "--------------\n",
      "Precision:  [73.86]\n",
      "Epoca:  87000\n",
      "--------------\n",
      "Precision:  [73.92]\n",
      "Epoca:  88000\n",
      "--------------\n",
      "Precision:  [73.88]\n",
      "Epoca:  89000\n",
      "--------------\n",
      "Precision:  [73.86]\n",
      "Epoca:  90000\n",
      "--------------\n",
      "Precision:  [73.9]\n",
      "Epoca:  91000\n",
      "--------------\n",
      "Precision:  [73.9]\n",
      "Epoca:  92000\n",
      "--------------\n",
      "Precision:  [73.9]\n",
      "Epoca:  93000\n",
      "--------------\n",
      "Precision:  [73.92]\n",
      "Epoca:  94000\n",
      "--------------\n",
      "Precision:  [73.94]\n",
      "Epoca:  95000\n",
      "--------------\n",
      "Precision:  [73.96]\n",
      "Epoca:  96000\n",
      "--------------\n",
      "Precision:  [73.94]\n",
      "Epoca:  97000\n",
      "--------------\n",
      "Precision:  [73.94]\n",
      "Epoca:  98000\n",
      "--------------\n",
      "Precision:  [73.96]\n",
      "Epoca:  99000\n",
      "--------------\n",
      "Precision:  [73.98]\n",
      "[[ 1.26197518e-01 -1.56046913e-01  4.45594934e-03  4.44132401e-03\n",
      "   5.13517677e-04  1.99942598e-03 -2.81683491e-02  1.99095012e-04\n",
      "   1.59130388e-03  6.69007102e-02  1.96629196e-01  2.49616339e-01\n",
      "   7.12189967e-04  9.78507978e-04  5.21537707e-04  1.61615322e-01\n",
      "   1.37860666e-01  4.40625291e-03  8.23154016e-04  1.06277676e-03\n",
      "   5.07488541e-01  8.76546938e-02  4.89520359e-02  1.33752974e-01\n",
      "   2.58641123e-01  3.96842257e-01  1.14500499e-03  3.24576911e-01\n",
      "   1.27777187e-01  1.48393313e-01  2.78177259e-03 -2.65265932e-01\n",
      "   8.06774307e-04  3.24156843e-03  3.25858406e-01  1.20255580e-03\n",
      "   7.48671708e-04  1.00090926e-01  3.99467576e-03  4.85531211e-02\n",
      "   3.66418130e-03  8.13899500e-02  7.90760670e-04  4.73443752e-02\n",
      "   1.41258530e-03  1.79643912e-01  3.55858883e-03  8.50664059e-02\n",
      "  -2.82648999e-03  1.05058029e-01  3.26605698e-03 -2.76057940e-02\n",
      "  -6.15391154e-02  4.94703988e-02  2.10433934e-01  4.63573482e-03\n",
      "   1.11453573e-01  1.82661511e-01 -6.16708673e-02  2.05906145e-01\n",
      "   1.05763836e-01  3.84032686e-03  4.03779929e-03 -1.02236612e-01\n",
      "   5.45210106e-04  9.33929282e-02  3.04512093e-03  9.61061857e-02\n",
      "   5.58939319e-03  9.05411438e-04  8.38474465e-02  2.79994919e-03\n",
      "   1.21960229e-03  2.77916369e-03 -1.76292180e-01  5.77856149e-02\n",
      "   8.95055017e-04  5.43584930e-02  2.68173524e-03  1.57701283e-01\n",
      "   1.49853867e-01  3.27322665e-03  5.38526177e-02  2.32401462e-01\n",
      "   2.53597229e-04 -1.19768567e-01  4.19911883e-03  1.33395008e-01\n",
      "   1.77244336e-03  8.22279822e-02  3.52691354e-03 -3.17301159e-02\n",
      "   3.83953643e-03  5.88109397e-04  4.29620697e-03  1.64072562e-03\n",
      "   2.72752097e-01 -4.72240429e-02  5.42589591e-01 -1.58833444e-01\n",
      "   3.70948970e-03  3.90379787e-02  1.06406677e-01  8.41103657e-04\n",
      "   1.72375046e-03 -3.23970586e-01 -1.60605233e-01 -6.94361121e-02\n",
      "  -8.73899070e-02 -4.26519463e-03 -2.99163403e-01 -5.29073451e-01\n",
      "   4.26987126e-03 -1.95606158e-01 -7.93487619e-03 -5.03237254e-02\n",
      "  -5.09117385e-02  2.97276544e-02 -5.65436603e-01  3.50588338e-03\n",
      "  -2.27667425e-01  2.37095347e-01  4.37962320e-03  2.32548447e-03\n",
      "  -2.73098233e-01  3.82399064e-03 -4.66692392e-01 -1.77845635e-01\n",
      "   3.41019235e-03  7.97851567e-04  3.79924462e-03 -5.55996292e-03\n",
      "  -2.32809265e-02  2.55156217e-03 -6.38872981e-02 -1.03406656e-02\n",
      "   4.45855986e-04  1.07141366e-01  4.53077253e-04 -9.05189258e-02\n",
      "  -2.68598785e-01 -1.40646602e-01 -1.20362026e-01  3.06939328e-03\n",
      "  -9.44727458e-04 -4.93842598e-02 -4.78317698e-02  7.06959045e-02\n",
      "  -2.31507133e-01 -1.80858369e-01 -3.39710365e-01  4.78395524e-03\n",
      "  -2.76632853e-01 -4.91551866e-01 -2.23664271e-01 -1.79262188e-01\n",
      "  -4.50755825e-04 -1.65518431e-01 -1.35202512e-01 -2.11567878e-01\n",
      "  -6.52544724e-02 -1.70968481e-01 -1.14750092e-01 -4.77640299e-01\n",
      "  -3.82580373e-01 -2.21923708e-01  2.27753063e-02 -1.60864266e-01\n",
      "   1.93948271e-03 -5.12149037e-01  3.60047468e-02  1.04684183e-01\n",
      "   3.35127126e-03 -2.71805922e-01  1.99494096e-03 -1.87924478e-01\n",
      "  -3.01994423e-02  7.74722394e-02  4.82235078e-03 -1.24965890e-01\n",
      "  -1.55239502e-01 -4.91222160e-02 -1.72276764e-02  2.33126465e-03\n",
      "  -1.52443303e-02  5.23463590e-04 -9.45435461e-02 -1.13464121e-01\n",
      "  -2.06556708e-01 -1.26801744e+00 -6.02099239e-01  7.66776977e-04\n",
      "  -1.62493545e-01 -1.04154000e-01  1.85565735e-03 -5.52814908e-02\n",
      "   5.87575344e-04  7.63566592e-04  7.65139543e-03 -8.57107487e-02\n",
      "  -2.53852553e-01 -9.42026033e-02 -5.42824023e-02  4.04765791e-03\n",
      "  -4.74216468e-01 -1.38707842e-01]]\n"
     ]
    }
   ],
   "source": [
    "alfa=0.001\n",
    "reg=0.002\n",
    "epochs=100000\n",
    "W=np.random.rand(1,x_train.shape[1])/x_train.shape[1]\n",
    "bias=np.random.rand(1,1)/x_train.shape[1]\n",
    "W2=W*1\n",
    "prec=0\n",
    "ep=0\n",
    "for epoch in range(epochs):\n",
    "    err=np.transpose(y_train)-sigmoid(bias+np.matmul(W2,np.transpose(x_train)))\n",
    "    dw=alfa*np.matmul(err,x_train)/x_train.shape[0]\n",
    "    W2+=dw\n",
    "    if epoch%1000==0:\n",
    "        y_pred=np.round(sigmoid(np.matmul(x_valid,np.transpose(W2))))\n",
    "        precision=100*(1-sum(abs(y_pred-y_valid))/y_pred.shape[0])\n",
    "        print('Epoca: ',epoch)\n",
    "        print('--------------')\n",
    "        print('Precision: ',precision)\n",
    "        if prec<precision:\n",
    "            prec=precision\n",
    "            ep=epoch\n",
    "            W=W2\n",
    "\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7856041 , -0.36576329, -0.58062278, ...,  0.57428338,\n",
       "         0.60062828, -0.48750515]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err=np.transpose(y_train)-sigmoid(bias+np.matmul(W2,np.transpose(x_train)))\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.980\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
