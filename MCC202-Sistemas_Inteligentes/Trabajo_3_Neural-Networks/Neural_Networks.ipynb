{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and upload all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import libraries and preprocess texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('data/shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jenazads/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop = stopwords.words('english') # Common words\n",
    "porter = PorterStemmer() # Getting root of words\n",
    "char3=stop[:17] # Getting 1st and 2nd person pronouns\n",
    "stop=stop[17:116]+stop[118:] \n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', ':)', ':)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stream=stream_docs(path='data/shuffled_movie_data.csv')\n",
    "docs, y = [], []\n",
    "for _ in range(50000):\n",
    "    text_aux, label =next(doc_stream)\n",
    "    text=tokenizer(text_aux)\n",
    "    docs.append(text)\n",
    "    y.append(label)\n",
    "    #print('\\n',tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing trash ad duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeDuplicates(listofElements):\n",
    "    # Create an empty list to store unique elements\n",
    "    noDupl = []\n",
    "    # Iterate over the original list and for each element\n",
    "    # add it to uniqueList, if its not already there.\n",
    "    for elem in listofElements:\n",
    "        if elem not in noDupl:\n",
    "            noDupl.append(elem)\n",
    "    \n",
    "    # Return the list of unique elements        \n",
    "    return noDupl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasifying positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# texto_dividido: a text splitted as a List of Words\n",
    "# posneg_dict: Positive and negative dictionary\n",
    "\n",
    "# Output:\n",
    "# COUNT_POSITIVE: # Of positive words according to the dictionary\n",
    "# COUNT_NEGATIVE: # Of negative words according to the dictionary\n",
    "def getPositiveNegativeCountWords(texto_dividido, posneg_dictionary):\n",
    "        # Count the positive words\n",
    "    COUNT_POSITIVE = 0\n",
    "    COUNT_NEGATIVE = 0\n",
    "    for word in texto_dividido:\n",
    "        try:\n",
    "            val = posneg_dictionary[word]\n",
    "            if val == 1:\n",
    "                COUNT_POSITIVE = COUNT_POSITIVE + 1\n",
    "            elif val == 0:\n",
    "                COUNT_NEGATIVE = COUNT_NEGATIVE + 1\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return (float(COUNT_POSITIVE), float(COUNT_NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2041\n",
      "1417\n",
      "4818\n",
      "3186\n"
     ]
    }
   ],
   "source": [
    "positives=[line.strip() for line in open('data/positive-words.txt')]\n",
    "positives = [porter.stem(w) for w in positives]\n",
    "print(len(positives))\n",
    "positives=removeDuplicates(positives)\n",
    "print(len(positives))\n",
    "negatives=[line.strip() for line in open('data/negative-words.txt')]\n",
    "negatives = [porter.stem(w) for w in negatives]\n",
    "print(len(negatives))\n",
    "negatives=removeDuplicates(negatives)\n",
    "print(len(negatives))\n",
    "pron12=stop[:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all words in data set from positive.csv and negative.csv, then will be compare with words in opinion english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['view', 'move', 'greatest', 'paul', 'brought', 'portray', 'marri', 'surprisingli', 'complex', 'joe', 'romant', 'anim', 'see', 'recent', 'stage', '8', 'york', 'disney', 'pace', 'awesom', 'adventur', 'dream', 'battl', 'die', 'memor', 'secret', 'atmospher', 'agre', 'manag', 'societi', 'stun', 'danc', 'subtl', 'recommend', 'centuri', 'romanc', 'tell', 'touch', 'plenti', 'power', 'languag', 'keep', 'fantast', 'popular', 'deep', 'truth', '7', 'follow', 'deserv', 'move', 'mark', 'geniu', 'danc', 'emot', 'impress', 'busi', 'today', 'creat', 'portray', 'it!', 'beauti', 'support', 'unlik', 'charm', 'america', 'terrif', 'talent', 'journey', 'cultur', 'uniqu', 'season', 'cold', 'thank', '9', 'earlier', 'match', 'western', 'bill', 'offic', 'person', 'edg', 'present', 'perfectli', 'older', 'favorit', 'tale', 'effect', 'situat', 'lead', 'bring', 'appreci', 'rich', 'famou', 'outsid', 'master', 'begin', 'masterpiec', 'brilliant', 'italian', 'variou']\n",
      "['lee', 'produc', 'incred', 'laugh', 'biggest', 'bottom', 'unbeliev', 'hair', 'ask', 'develop', 'villain', 'slasher', 'begin', 'footag', 'tri', 'bore', 'plain', 'pain', 'weird', 'bare', 'valu', 'posit', 'excus', 'lame', 'mess', 'appar', 'nuditi', 'rent', 'predict', 'laughabl', 'grade', 'pay', 'worst', 'apart', 'pointless', 'speak', 'island', '30', 'explain', 'fire', 'reason', 'channel', 'total', 'avoid', 'whatev', 'fake', 'dumb', 'twist', 'alien', 'joke', 'poorli', 'yeah', 'edit', 'flat', 'van', 'cheap', 'suddenli', 'unless', 'expect', 'wait', 'hardli', 'shoot', 'badli', 'monster', 'sick', 'consid', 'kill', 'walk', 'parti', 'terribl', 'need', 'produc', 'trash', 'bother', 'effort', 'cheesi', 'sit', 'project', 'spend', 'miss', 'horribl', 'credit', 'cover', 'sound', 'disappoint', 'premis', 'maker', 'filmmak', 'spent', 'result', 'disappoint', 'write', 'sorri', 'sit', 'appear', 'end', 'decid', 'fail', 'shame', 'brain']\n"
     ]
    }
   ],
   "source": [
    "P2=100 # Number of features\n",
    "\n",
    "#We separate common words\n",
    "positive2 = pd.read_csv('data/positive.csv', index_col=0)\n",
    "positive2_i = positive2.index.values[:1000]\n",
    "stop2 = stopwords.words('english')\n",
    "positive2_i=set(positive2_i).difference(stop2)\n",
    "\n",
    "\n",
    "negative2 = pd.read_csv('data/negative.csv', index_col=0)\n",
    "negative2_i = negative2.index.values[:1000]\n",
    "negative2_i=set(negative2_i).difference(stop2)\n",
    "\n",
    "# We proced to delete repeated words.\n",
    "positive2=set(positive2_i).difference(negative2_i)\n",
    "negative2=set(negative2_i).difference(positive2_i)\n",
    "positive2=list(positive2)[:P2]\n",
    "negative2=list(negative2)[:P2]\n",
    "\n",
    "positive2 = [porter.stem(w) for w in positive2]\n",
    "negative2 = [porter.stem(w) for w in negative2]\n",
    "\n",
    "print(positive2)\n",
    "print(negative2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "(50000, 6)\n"
     ]
    }
   ],
   "source": [
    "N=50000\n",
    "\n",
    "X=np.zeros((N,6))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    if idx%1000==0:\n",
    "        print(idx)\n",
    "    X[idx,5]+=np.log(len(com)) #len: char 6\n",
    "    for word in com:\n",
    "        X[idx,3]+=char3.count(word) #pronoun: char 4\n",
    "        if word=='!':\n",
    "            X[idx,4]=1 #! simbol: char 5\n",
    "        if (word=='no' or word=='not'):\n",
    "            X[idx,2]=1 #! simbol: char 5\n",
    "        X[idx,0]+=positives.count(word) #positive words : char 1\n",
    "        X[idx,1]+=negatives.count(word) #negative words : char 2\n",
    "    idx+=1\n",
    "\n",
    "X2=np.zeros((N,2*P2))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    #if com.count('terrific'):\n",
    "        #print('foundddddd')\n",
    "    #if idx%1000==0:\n",
    "    #    print(idx)\n",
    "    idx2=0\n",
    "    for word in positive2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    #print(idx2)\n",
    "    for word in negative2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    idx+=1\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to combine all data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=np.concatenate((X, X2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.asarray(y)\n",
    "y=y.reshape(y.shape[0],1)\n",
    "x_train=X_data[:40000]\n",
    "x_valid=X_data[40000:45000]\n",
    "x_test=X_data[45000:50000]\n",
    "y_train=y[:40000]\n",
    "y_valid=y[40000:45000]\n",
    "y_test=y[45000:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def sigmoid_derivate(z):\n",
    "    s = sigmoid(z) *(1 - sigmoid(z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivate(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivate(self.output), self.weights2.T) * sigmoid_derivate(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Epoca:  0\n",
      "Precision:  [50.01279178]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenazads/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Epoca:  100\n",
      "Precision:  [48.5175]\n",
      "--------------\n",
      "Epoca:  200\n",
      "Precision:  [51.6375]\n",
      "--------------\n",
      "Epoca:  300\n",
      "Precision:  [51.62875]\n",
      "--------------\n",
      "Epoca:  400\n",
      "Precision:  [51.64625]\n",
      "--------------\n",
      "Epoca:  500\n",
      "Precision:  [51.64375]\n",
      "--------------\n",
      "Epoca:  600\n",
      "Precision:  [48.3475]\n",
      "--------------\n",
      "Epoca:  700\n",
      "Precision:  [48.33125027]\n",
      "--------------\n",
      "Epoca:  800\n",
      "Precision:  [51.655]\n",
      "--------------\n",
      "Epoca:  900\n",
      "Precision:  [51.65]\n",
      "--------------\n",
      "Epoca:  1000\n",
      "Precision:  [51.6475]\n",
      "--------------\n",
      "Epoca:  1100\n",
      "Precision:  [51.6375]\n",
      "--------------\n",
      "Epoca:  1200\n",
      "Precision:  [51.64125]\n",
      "--------------\n",
      "Epoca:  1300\n",
      "Precision:  [51.64625]\n",
      "--------------\n",
      "Epoca:  1400\n",
      "Precision:  [51.64125]\n",
      "--------------\n",
      "Epoca:  1500\n",
      "Precision:  [48.34375]\n",
      "--------------\n",
      "Epoca:  1600\n",
      "Precision:  [51.66125]\n",
      "--------------\n",
      "Epoca:  1700\n",
      "Precision:  [51.6625]\n",
      "--------------\n",
      "Epoca:  1800\n",
      "Precision:  [51.6625]\n",
      "--------------\n",
      "Epoca:  1900\n",
      "Precision:  [48.33125]\n",
      "--------------\n",
      "Epoca:  2000\n",
      "Precision:  [48.3325]\n",
      "--------------\n",
      "Epoca:  2100\n",
      "Precision:  [51.685]\n",
      "--------------\n",
      "Epoca:  2200\n",
      "Precision:  [51.68625]\n",
      "--------------\n",
      "Epoca:  2300\n",
      "Precision:  [51.68]\n",
      "--------------\n",
      "Epoca:  2400\n",
      "Precision:  [51.67]\n",
      "--------------\n",
      "Epoca:  2500\n",
      "Precision:  [51.6825]\n",
      "--------------\n",
      "Epoca:  2600\n",
      "Precision:  [48.32375]\n",
      "--------------\n",
      "Epoca:  2700\n",
      "Precision:  [48.31625]\n",
      "--------------\n",
      "Epoca:  2800\n",
      "Precision:  [48.31625]\n",
      "--------------\n",
      "Epoca:  2900\n",
      "Precision:  [48.32511336]\n",
      "--------------\n",
      "Epoca:  3000\n",
      "Precision:  [48.30375]\n",
      "--------------\n",
      "Epoca:  3100\n",
      "Precision:  [48.33751967]\n",
      "--------------\n",
      "Epoca:  3200\n",
      "Precision:  [48.34875224]\n",
      "--------------\n",
      "Epoca:  3300\n",
      "Precision:  [51.70051219]\n",
      "--------------\n",
      "Epoca:  3400\n",
      "Precision:  [51.69]\n",
      "--------------\n",
      "Epoca:  3500\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  3600\n",
      "Precision:  [51.66625]\n",
      "--------------\n",
      "Epoca:  3700\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  3800\n",
      "Precision:  [48.335]\n",
      "--------------\n",
      "Epoca:  3900\n",
      "Precision:  [51.66375]\n",
      "--------------\n",
      "Epoca:  4000\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  4100\n",
      "Precision:  [48.3325]\n",
      "--------------\n",
      "Epoca:  4200\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  4300\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  4400\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  4500\n",
      "Precision:  [48.3325]\n",
      "--------------\n",
      "Epoca:  4600\n",
      "Precision:  [51.665]\n",
      "--------------\n",
      "Epoca:  4700\n",
      "Precision:  [51.66875]\n",
      "--------------\n",
      "Epoca:  4800\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  4900\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  5000\n",
      "Precision:  [51.66625]\n",
      "--------------\n",
      "Epoca:  5100\n",
      "Precision:  [48.33125]\n",
      "--------------\n",
      "Epoca:  5200\n",
      "Precision:  [51.66875]\n",
      "--------------\n",
      "Epoca:  5300\n",
      "Precision:  [51.66625]\n",
      "--------------\n",
      "Epoca:  5400\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  5500\n",
      "Precision:  [51.66875]\n",
      "--------------\n",
      "Epoca:  5600\n",
      "Precision:  [48.33]\n",
      "--------------\n",
      "Epoca:  5700\n",
      "Precision:  [51.6725]\n",
      "--------------\n",
      "Epoca:  5800\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  5900\n",
      "Precision:  [48.3325]\n",
      "--------------\n",
      "Epoca:  6000\n",
      "Precision:  [51.6725]\n",
      "--------------\n",
      "Epoca:  6100\n",
      "Precision:  [51.6675]\n",
      "--------------\n",
      "Epoca:  6200\n",
      "Precision:  [51.67]\n",
      "--------------\n",
      "Epoca:  6300\n",
      "Precision:  [51.67375]\n",
      "--------------\n",
      "Epoca:  6400\n",
      "Precision:  [51.67375]\n",
      "--------------\n",
      "Epoca:  6500\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  6600\n",
      "Precision:  [48.85375]\n",
      "--------------\n",
      "Epoca:  6700\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  6800\n",
      "Precision:  [51.6725]\n",
      "--------------\n",
      "Epoca:  6900\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  7000\n",
      "Precision:  [48.32625]\n",
      "--------------\n",
      "Epoca:  7100\n",
      "Precision:  [51.67375]\n",
      "--------------\n",
      "Epoca:  7200\n",
      "Precision:  [51.67375]\n",
      "--------------\n",
      "Epoca:  7300\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  7400\n",
      "Precision:  [48.32875]\n",
      "--------------\n",
      "Epoca:  7500\n",
      "Precision:  [51.67]\n",
      "--------------\n",
      "Epoca:  7600\n",
      "Precision:  [51.6725]\n",
      "--------------\n",
      "Epoca:  7700\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  7800\n",
      "Precision:  [51.67625]\n",
      "--------------\n",
      "Epoca:  7900\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  8000\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  8100\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  8200\n",
      "Precision:  [48.33125001]\n",
      "--------------\n",
      "Epoca:  8300\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  8400\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  8500\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  8600\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  8700\n",
      "Precision:  [48.325]\n",
      "--------------\n",
      "Epoca:  8800\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  8900\n",
      "Precision:  [51.67125]\n",
      "--------------\n",
      "Epoca:  9000\n",
      "Precision:  [48.8562497]\n",
      "--------------\n",
      "Epoca:  9100\n",
      "Precision:  [51.67625]\n",
      "--------------\n",
      "Epoca:  9200\n",
      "Precision:  [51.67625]\n",
      "--------------\n",
      "Epoca:  9300\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  9400\n",
      "Precision:  [48.32375]\n",
      "--------------\n",
      "Epoca:  9500\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  9600\n",
      "Precision:  [51.67375]\n",
      "--------------\n",
      "Epoca:  9700\n",
      "Precision:  [51.6725]\n",
      "--------------\n",
      "Epoca:  9800\n",
      "Precision:  [51.675]\n",
      "--------------\n",
      "Epoca:  9900\n",
      "Precision:  [51.67625]\n"
     ]
    }
   ],
   "source": [
    "alfa=0.001\n",
    "reg=0.002\n",
    "epochs=10000\n",
    "W=np.random.rand(1,x_train.shape[1])/x_train.shape[1]\n",
    "nn = NeuralNetwork(x_train, y_train)\n",
    "prec=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "    if epoch%100==0:\n",
    "        y_pred=nn.output\n",
    "        precision=100*(1-sum(abs(y_pred-y_train))/y_pred.shape[0])\n",
    "        print('--------------')\n",
    "        print('Epoca: ',epoch)\n",
    "        print('Precision: ',precision)\n",
    "        if prec<precision:\n",
    "            prec=precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47507968, -0.53871841, -0.53491084, ...,  0.48429609,\n",
       "         0.48527271, -0.51130759]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err=np.transpose(y_train)-sigmoid(np.matmul(W,np.transpose(x_train)))\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.676\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
